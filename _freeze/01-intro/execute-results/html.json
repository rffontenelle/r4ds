{
  "hash": "f8884c10725afe96e568105643e1f6ff",
  "result": {
    "markdown": "# Introducción\n\nLa ciencia de datos (_data science_) es una disciplina fascinante que te permite convertir datos sin procesar en entendimiento, comprensión y conocimiento. El objetivo de este libro es ayudarte a aprender las herramientas más importantes para que puedas hacer ciencia de datos en R. Luego de leerlo, tendrás las herramientas para enfrentar una gran variedad de desafíos propios de esta área, usando las mejores partes de R.  \n\n## Lo que aprenderás\n\nLa ciencia de datos es un campo muy amplio y no hay manera de que puedas dominarlo leyendo un solo libro. El objetivo de este, en particular, es entregarte una base sólida acerca de las herramientas más importantes. Nuestro modelo sobre cuáles son las herramientas necesarias para un proyecto típico de ciencia de datos se ve así:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](diagrams_w_text_as_path/es/data-science.svg){width=75%}\n:::\n:::\n\n\nPrimero, debes __importar__ tus datos hacia R. Típicamente, esto implica tomar datos que están guardados en un archivo, base de datos o API y cargarlos como _data frame_ en R. Si no puedes llevar tus datos a R, no puedes hacer ciencia de datos con él.\n\nUna vez que has importado los datos, es una buena idea __ordenarlos__. Ordenar los datos significa guardarlos de una manera consistente que haga coincidir la semántica del set de datos con la manera en que está guardado. En definitiva, cuando tus datos están ordenados, cada columna es una variable y cada fila una observación. Tener datos ordenados es importante porque si su estructura es consistente, puedes enfocar tus esfuerzos en las preguntas sobre los datos y no en luchar para que estos tengan la forma necesaria para diferentes funciones.\n\nCuando tus datos están ordenados, un primer paso suele ser __transformarlos__. La transformación implica reducir las observaciones a aquellas que sean de interés (como todas las personas de una ciudad o todos los datos del último año), crear nuevas variables que sean funciones de variables ya existentes (como calcular la rapidez a partir de la velocidad y el tiempo) y calcular una serie de estadísticos de resumen (como recuentos y medias). Juntos, a ordenar y transformar, se les llama __manejar o domar__ los datos, porque hacer que estos tengan la forma con la que es natural trabajarlos, suele sentirse como una lucha.\n\nUna vez que tienes los datos ordenados con las variables que necesitas, hay dos principales fuentes generadoras de conocimiento: la visualización y el modelado. Ambas tienen fortalezas y debilidades complementarias, por lo que cualquier análisis real iterará entre ellas varias veces.\n\nLa __visualización__ es una actividad humana fundamental. Una buena visualización te mostrará cosas que no esperabas o hará surgir nuevas preguntas acerca de los datos. También puede darte pistas acerca de si estás haciendo las preguntas equivocadas o si necesitas recolectar datos diferentes. Las visualizaciones pueden sorprenderte, pero no escalan particularmente bien, ya que requieren ser interpretadas por una persona.\n\nLos __modelos__ son herramientas complementarias a la visualización. Una vez que tus preguntas son lo suficientemente precisas, puedes utilizar un modelo para responderlas. Los modelos son herramientas matemáticas o computacionales, por lo que generalmente escalan bien. Incluso cuando no lo hacen, resulta más económico comprar más computadores que comprar más cerebros. Sin embargo, cada modelo tiene supuestos y, debido a su propia naturaleza, un modelo no puede cuestionar sus propios supuestos. Esto significa que un modelo, por definición, no puede sorprenderte.\n\nEl último paso de la ciencia de datos es la __comunicación__, una parte crítica de cualquier proyecto de análisis de datos. No importa qué tan bien tus modelos y visualizaciones te hayan permitido entender tus datos, a menos que también puedas comunicar esos resultados a otras personas.\n\nAlrededor de todas estas herramientas se encuentra la __programación__. La programación es una herramienta transversal que usarás en todas las partes de tu proyecto. No necesitas ser una personas experta en programación para hacer ciencia de datos, pero aprender más sobre ella es una gran ventaja porque te permite automatizar tareas recurrentes y resolver problemas con mayor facilidad.\n\nEn cualquier proyecto de ciencia de datos tendrás que ocupar estas herramientas, pero en muchos casos estas no serán suficientes. Hay un regla aproximada de 80-20 en juego: puedes enfrentar alrededor del 80 % de cualquier proyecto usando las herramientas que aprenderás en este libro, pero necesitarás utilizar otras para abordar el 20 % restante. A lo largo del libro te iremos señalando recursos donde puedes aprender más.\n\n## Cómo está organizado este libro\n\nLa descripción anterior de las herramientas propias de las ciencia de datos está organizada aproximadamente de acuerdo al orden en que usualmente se usan en el análisis (aunque, por supuesto, tendrás que iterar entre ellas múltiples veces). Sin embargo, en nuestra experiencia esa no es la mejor manera de aprenderlas:\n\n* Partir con la ingesta y orden de los datos no es lo óptimo porque el 80%\n  del tiempo es un proceso rutinario y aburrido y el 20% restante es extraño y frustrante.\n  ¡No es un buen lugar para aprender un tema nuevo! En cambio, partiremos\n  con la visualización y transformación de datos que ya han sido importados\n  y ordenados. De esta manera, cuando tengas que importar y ordenar tus\n  propios datos, tu motivación se mantendrá alta porque sabrás que ese\n  sufrimiento vale la pena.\n\n* Algunos temas se explican mejor con otras herramientas. Por ejemplo, creemos\n  que es más fácil entender qué es un modelo si ya sabes sobre visualización,\n  datos ordenados y programación.\n\n* Las herramientas de programación no son necesariamente interesantes en sí mismas;\n  sin embargo, te permiten enfrentar problemas desafiantes. Te entregaremos una\n  selección de herramientas de programación en la mitad del libro y luego verás\n  cómo se pueden combinar con las herramientas propias de la ciencia de datos\n  para enfrentar problemas de modelado interesantes.\n\nEn cada capítulo hemos tratado de mantener un patrón similar: partir con algunos ejemplos motivantes que te permitan ver el panorama completo y luego sumergirnos en los detalles. Cada sección del libro incluye ejercicios que te ayudarán a practicar lo que has aprendido. Pese a que puede ser tentador saltarse los ejercicios, no hay mejor manera de aprender que practicar con problemas reales.\n\n## Qué no vas a aprender\n\nHay algunos temas importantes que este libro no aborda. Creemos que es importante mantenernos enfocados con determinación en los aspectos esenciales, con el fin de que puedas ponerte en marcha lo más rápido posible. Eso implica que este libro no puede abordar todos los temas importantes.\n\n### Big data\n\nEste libro se enfoca con orgullo en conjuntos de datos pequeños procesables en la memoria de tu computadora. Este es el lugar adecuado para partir, ya que no es posible que te enfrentes a datos de gran tamaño sin antes haber tenido experiencia con otros más pequeños. Las herramientas que aprenderás en este libro permiten manejar fácilmente datos de cientos de megabytes y, con un poco de cuidado, normalmente podrías hacerlas funcionar con 1 o 2 Gb de datos. Si habitualmente trabajas con datos más grandes (por ejemplo, 10-100 Gb), sería bueno que aprendieras sobre  [data.table](https://github.com/Rdatatable/data.table). Este libro no enseña `data.table`, ya que su interfaz concisa hace que sea difícil de aprender por las pocas pistas lingüísticas que entrega. Sin embargo, si trabajas con datos grandes, la ventaja en términos de rendimiento hace que valga la pena el esfuerzo extra que requiere aprenderlo.\nSi tus datos son más grandes que eso, es importante que consideres cuidadosamente si tu problema de _big data_ no es, en realidad, un problema de datos pequeños oculto. Si bien los datos completos pueden ser grandes, muchas veces los necesarios para responder una pregunta específica son menos. Puede que encuentres un subconjunto, una submuestra o un resumen de datos que sí caben en la memoria y que de todos modos te permiten responder la pregunta que te interesa. El desafío en este caso es encontrar los datos pequeños adecuados, lo que usualmente requiere muchas iteraciones.\n\nOtra posibilidad es que tu problema de _big data_ sea realmente una suma de problemas de datos pequeños. Puede que cada uno de estos problema individuales quepa en la memoria; el problema es que tienes millones de ellos. Por ejemplo, puedes querer ajustar un modelo para cada persona de tu conjunto de datos. Eso sería trivial si tuvieras solo 10 o 100 personas, pero no si tienes un millón. Afortunadamente, cada problema es independiente del resto (una configuración a la que a veces se le llama de manera vergonzosa _paralela_), por lo que solo necesitas un sistema (como Hadoop o Spark) que te permita enviar diferentes sets de datos a diferentes computadoras para procesarlos. Una vez que hayas resuelto cómo responder la pregunta para un subset de datos usando las herramientas descritas en este libro, podrás aprender otras nuevas como `sparklyr`, `RHIPE` y `ddr` para responder la pregunta para todo el _dataset_.\n\n### Python, Julia y amigos\n\nEn este libro no aprenderás nada sobre Python, Julia u otros lenguajes de programación útiles para hacer ciencia de datos. No es que creamos que estas herramientas sean malas. ¡No lo son! En la práctica, la mayoría de los equipos de ciencia de datos utilizan una mezcla de lenguajes, habitualmente al menos R y Python.\n\nSin embargo, creemos fuertemente que es preferible dominar una sola herramienta a la vez. Mejorarás más rápido si te sumerges en un tema con profundidad, en vez de dispersarte entre muchos temas distintos. Esto no quiere decir que solo tengas que saber una cosa, solamente que aprenderás más rápido si te enfocas en una a la vez. Debes tratar de aprender cosas nuevas a lo largo de tu carrera, pero asegúrate de que tu entendimiento sea sólido antes de moverte hacia el siguiente tema interesante.\n\nCreemos que R es un gran lugar para empezar tu camino en la ciencia de datos, ya que es un ambiente diseñado desde las bases hacia arriba para apoyarla. R no es solo un lenguaje de programación, sino también un ambiente interactivo para hacer ciencia de datos. Para apoyar esta interacción, R es mucho más flexible que muchos de sus equivalentes. Si bien esta flexibilidad tiene desventajas, su lado positivo es que permite desarrollar con facilidad gramáticas que se ajustan a las distintas partes de la ciencia de datos. Estos mini-lenguajes te ayudan a pensar los problemas como científico/a de datos, al tiempo que apoyan una interacción fluida entre tu cerebro y la computadora.\n\n### Datos no rectangulares\n\nEste libro se enfoca exclusivamente en datos rectangulares, esto es, en conjuntos de valores que están asociados cada uno con una variable y una observación. Hay muchos conjuntos de datos que no se ajustan naturalmente a este paradigma, los que incluyen imágenes, sonidos, árboles y texto. Sin embargo, los _data frames_ rectangulares son tan comunes en la ciencia y en la industria, que creemos que son un buen lugar para iniciar tu camino en la ciencia de datos.\n\n### Confirmación de hipótesis\n\nEs posible dividir el análisis de datos en dos áreas: generación de hipótesis y confirmación de hipótesis (a veces llamada análisis confirmatorio). El foco de este libro está puesto decididamente en la generación de hipótesis o exploración de datos. Acá mirarás con profundidad los datos y, en combinación con tu propio conocimiento, generarás muchas hipótesis interesantes que ayudarán a explicar por qué se comportan como lo hacen. Evaluarás las hipótesis informalmente, usando tu escepticismo para cuestionar los datos de distintas maneras.\n\nEl complemento de la generación de hipótesis es la confirmación de hipótesis. Esta última es difícil por dos razones:\n\n\n1.  Necesitas un modelo matemático con el fin de generar predicciones falseables.\n Esto usualmente requiere considerable sofisticación estadística.\n\n1.  Cada observación puede ser utilizada una sola vez para confirmar una hipótesis.\n En el momento en que la usas más de una vez ya estás de vuelta haciendo análisis\n exploratorio. Esto quiere decir que para hacer confirmación de hipótesis tienes\n que haber \"pre-registrado\" (es decir, haber escrito con anticipación) tu plan\n de análisis y no desviarte de él incluso cuando hayas visto tus datos. Hablaremos\n un poco acerca de estrategias que puedes utilizar para hacer esto más fácil en el\n capítulo sobre [modelos](#model-intro).\n\nEs habitual pensar en el modelado como una herramienta para la confirmación de hipótesis y a la visualización como una herramienta para la generación de hipótesis. Sin embargo, esa es una falsa dicotomía: los modelos son  utilizados usualmente para la exploración y, con un poco de cuidado, puedes usar la visualización para confirmar una hipótesis. La diferencia clave es qué tan seguido mires cada observación: si la miras solo una vez es confirmación; si la miras más de una vez es exploración.\n\n## Prerrequisitos\n\nHemos asumido algunas cosas respecto de lo que ya sabes con el fin de poder sacar mayor provecho de este libro. Tienes que tener una _literacidad_ numérica general y sería útil que tuvieses algo de experiencia programando. Si nunca has programado antes y puedes leer en inglés, [Hands on Programming with R](https://rstudio-education.github.io/hopr/) escrito por Garrett podría ser un buen acompañamiento para este libro.\n\nHay cinco cosas que necesitas para poder ejecutar el código incluido en este libro: R, RStudio, una colección de paquetes llamada __tidyverse__, el paquete __datos__ (que incluye los datos en español se se utilizan en los ejemplos y ejercicios) y una serie de otros paquetes. Los paquetes son la unidad fundamental de código reproducible de R. Incluyen funciones reutilizables, la documentación que describe cómo usarlas y datos de muestra.\n\n### R\n\nPara descargar R debes acceder a CRAN, llamado así por sus siglas en inglés: the **c**omprehensive **R** **a**rchive **n**etwork. CRAN está compuesto de una serie de servidores espejo repartidos alrededor del mundo y es utilizado para distribuir tanto R como los paquetes de R. No es necesario que intentes elegir un servidor que esté cerca tuyo: en su lugar, puedes utilizar el servidor en la nube, <https://cloud.r-project.org>, que automáticamente lo identifica por ti.\n\nUna vez al año sale una nueva versión importante de R y hay entre 2 y 3 ediciones menores en ese período. Es una buena idea actualizarlo regularmente. El proceso puede ser un poco engorroso, especialmente en el caso de las versiones mayores, que requieren que reinstales todos los paquetes que ya tienes. Sin embargo, no hacerlo puede ser peor. Para este libro, asegúrate de tener al menos la versión 3.5.\n\n### RStudio\n\nRStudio es un ambiente de desarrollo integrado (o IDE, por su sigla en inglés: Integrated Development Environment) para programar en R. Puedes descargarlo e instalarlo desde  <http://www.rstudio.com/download>. RStudio se actualiza un par de veces al año. Cuando haya una nueva versión disponible, el mismo programa te lo hará saber. Es una buena idea mantenerlo actualizado para que puedas aprovechar las mejores y más recientes características. Para este libro, asegúrate de tener al menos la versión 1.0.0.\n\nCuando abras RStudio, verás en la interfaz dos regiones clave:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](diagrams_w_text_as_path/es/rstudio-console.svg){width=75%}\n:::\n:::\n\n\nPor ahora, todo lo que tienes que saber es que el código de R se escribe en la Consola y que hay que presionar Enter para ejecutarlo. ¡Aprenderás más a medida que avancemos!\n\n### El Tidyverse\n\nEs necesario que instales también algunos paquetes de R. Un __paquete__ es una colección de funciones, datos y documentación que permite extender las capacidades de R base. Los paquetes son clave para usar R de manera exitosa. La mayoría de los paquetes que aprenderás a usar en este libro son parte del llamado \"Tidyverse\". Los paquetes del Tidyverse comparten una filosofía acerca de los datos y la programación en R, y están diseñados para trabajar juntos con naturalidad. Su nombre viene de la palabra en inglés \"tidy\", que quiere decir \"ordenado\".\n\nPuedes instalar el __tidyverse__ completo con una sola línea de código:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"tidyverse\")\n```\n:::\n\n\nEscribe en tu computadora esa línea de código en la consola y luego presiona Enter para ejecutarla. R descargará los paquetes de CRAN y los instalará en tu computadora. Si tienes problemas durante la instalación, asegúrate que tienes conexión a Internet y que  <https://cloud.r-project.org/> no está bloqueado por tu firewall o proxy.\n\nNo podrás usar las funciones, objetos y archivos de ayuda de un paquete hasta que lo hayas cargado con `library()`. Una vez que has instalado un paquete, puedes cargarlo con la función `library()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n:::\n\n\nEste mensaje te indica que el tidyverse está cargando los paquetes __ggplot2__, __tibble__, __tidyr__, __readr__, __purrr__, __dplyr__, __stringr__ y __forcats__. Estos son considerados el __corazón__ del Tidyverse porque los usarás prácticamente en cualquier análisis.\n\nLos paquetes del Tidyverse cambian con bastante frecuencia. Puedes ver si existen actualizaciones disponibles y opcionalmente instalarlas ejecutando `tidyverse_update()`.\n\n### El paquete `datos`\n\nCon el fin de que este libro sea más accesible para el público hispanoparlante, además de la traducción del texto se han traducido los datos que se utilizan en los ejemplos y ejercicios.\n\nEl paquete `datos` se encuentra disponible en CRAN y puedes instalarlo ejecutando el siguiente código:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"datos\")\n```\n:::\n\n\n### Otros paquetes\n\nExisten muchas otros paquetes excelentes que no son parte del __tidyverse__ porque resuelven problemas de otros ámbitos o porque los principios en los que se basa su diseño son distintos. Esto no los hace mejores o peores, solo diferentes. En otras palabras, el complemento del __tidyverse__ no es el _messyverse_ (del inglés _messy_, desordenado), sino muchos otros universos de paquetes interrelacionados. A medida que te enfrentes a más proyectos de ciencia de datos con R, aprenderás sobre nuevos paquetes y nuevas formas de pensar los datos.\n\n\n## Ejecutar código en R\n\nEn la sección anterior te mostramos algunos ejemplos de cómo ejecutar código en R. En el libro, el código se ve así:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n\n```{.r .cell-code}\n#> [1] 3\n```\n:::\n\n\nSi ejecutas el mismo código en tu consola local, se verá así:\n\n```\n> 1 + 2\n[1] 3\n```\n\nHay dos diferencias principales. La primera, es que en tu consola debes escribir el código después del signo `>`, llamado _prompt_; en el libro no te mostraremos el _prompt_. La segunda, es que en el libro el _output_, es decir, el resultado de ejecutar el código, está comentado: `#>`. En tu consola, el output aparecerá directamente después del código. Estas dos diferencias implican que, como esta es una versión electrónica del libro, puedes copiar directamente el código que aparece acá y pegarlo en tu consola.\n\nA lo largo del libro usaremos una serie consistente de convenciones para referirnos al código:\n\n* Las funciones están escritas en una fuente para código y seguidas de paréntesis.\n La primera vez que son mencionadas ofreceremos una traducción al español:\n `sum()` (del inglés _suma_) o `mean()` (del inglés _media_).\n\n* Otros tipos de objetos de R (como datos o argumentos de funciones)\n estarán en fuente para código, pero sin paréntesis: `vuelos` o `x`.\n\n* Si queremos dejar claro de qué paquete viene un objeto, usaremos el nombre\n del paquete seguido de doble dos puntos: `dplyr::mutate()` o `datos::paises`.\n Esto también es válido para el código de R.\n\n## Pedir ayuda y aprender más\n\nEste libro no es una isla. No existe ningún recurso que por sí mismo te permita dominar R. A medida que empieces a aplicar las técnicas descritas en este libro a tus propios datos te encontrarás con preguntas que acá no respondemos. En esta sección se describen algunas sugerencias sobre cómo pedir ayuda y cómo seguir aprendiendo.\n\nSi en algún momento ya no puedes avanzar, empieza buscando en Google. Usualmente, agregar \"R\" a tu búsqueda es suficiente para que se restrinja solo a resultados relevantes. Si lo que encuentras no es útil, probablemente sea porque no hay resultados disponibles en español. Google es particularmente útil para los mensajes de error. Si te aparece uno y no tienes idea de lo que significa, ¡prueba buscando en Google! Lo más probable es que alguien más se haya confundido con ese mensaje en el pasado y que haya ayuda en la web. Si el error te aparece en español u otro idioma, ejecuta en la consola `Sys.setenv(LANGUAGE = \"en\")` y luego vuelve a ejecutar el código. Es más probable que encuentres ayuda si el error que arroja R está en inglés.\n\nSi Google no ayuda, prueba con la versión en español de [Stack Overflow](https://es.stackoverflow.com/). Parte dedicando un tiempo a buscar si existe ya una respuesta a tu pregunta agregando `[R]` a tu búsqueda para restringir los resultados a preguntas y respuestas que usen R. Si no encuentras nada útil, prepara un ejemplo reproducible o __reprex__.  Un buen _reprex_ hace más fácil que otras personas te puedan ayudar y al prepararlo probablemente resuelvas el problema por tu cuenta.\n\nHay tres cosas que debes incluir para hacer que tu ejemplo sea reproducible: los paquetes necesarios, datos y código.\n\n1.  Los **paquetes** deben ser cargados al inicio del _script_ (que es\n como se le llama a la secuencia de comandos) para que sea fácil ver cuáles\n se necesitan para el ejemplo. Es una buena oportunidad para chequear que estás\n utilizando la última versión de cada paquete. Es posible que hayas descubierto\n un error (o _bug_, en inglés) que ya fue resuelto desde que instalaste\n el paquete. Para los paquetes del __tidyverse__, la manera más fácil de hacerlo\n es ejecutando `tidyverse_update()`.\n\n1.  La manera más simple de incluir **datos** en una pregunta es usar `dput()`\n para generar el código de R que los recree. Por ejemplo, para recrear\n el conjunto de datos `mtautos` en R, tendríamos que realizar los siguientes\n pasos.\n\n  1. Cargar el paquete que contiene los datos: `library(datos`)\n  2. Ejecutar `dput(mtautos)` en R\n  3. Copiar el output\n  4. En tu script reproducible, escribir `mtautos <- ` y luego pegar lo copiado.\n\n  Trata de buscar el subconjunto más pequeño de tus datos que\n te permita mostrar tu problema.\n\n1.  Dedica tiempo a asegurarte que tu **código** puede ser fácilmente leído\n por otras personas:\n\n  * Asegúrate de haber utilizado espacios y que los nombre de tus variables\n son a la vez concisos e informativos.\n\n  * Realiza comentarios que indiquen dónde se encuentra el problema.\n\n  * Haz lo posible por remover todo lo que no esté relacionado con el problema.\n Mientras más breve tu código, más fácil de entender y más fácil de arreglar.\n\nFinalmente, revisa que tu ejemplo es efectivamente reproducible. Para ello, inicia una nueva sesión de R y copia y pega tu script ahí.\n\nTambién deberías dedicar tiempo a prepararte para resolver problemas por tu cuenta antes de que ocurran. Invertir un poco de tiempo cada día aprendiendo R te entrega grandes beneficios a largo plazo. Una manera es siguiendo lo que hacen Hadley, Garrett y todas las personas de RStudio en el [blog de RStudio](https://blog.rstudio.org). Ahí es donde se publican anuncios sobre nuevos paquetes, nuevas características del entorno de desarrollo integrado (IDE) y talleres presenciales. También te podría interesar seguir a Hadley ([\\@hadleywickham](https://twitter.com/hadleywickham)) o a Garrett ([\\@statgarrett](https://twitter.com/statgarrett)) en Twitter, o a la cuenta  [\\@rstudiotips](https://twitter.com/rstudiotips) para mantenerte al día sobre las nuevas características RStudio.\n\nPara estar al tanto acerca de la comunidad de R en general, puedes leer  <http://www.r-bloggers.com>: que agrupa cerca de 500 blogs sobre R de todas partes del mundo, algunos incluso en español. Si tienes cuenta en Twitter, puedes seguir el hashtag `#rstatsES` (en español) o `#rstats` (en inglés). Twitter es una de las herramientas clave que usa Hadley para mantenerse al día sobre los nuevos desarrollos de la comunidad.\n\n## Agradecimientos\n\nEste libro no es solo el producto de Hadley y Garrett, sino el resultado de muchas conversaciones (en persona y en línea) que hemos tenido con gente de la comunidad de R. Hay algunas personas a las que nos gustaría agradecer de manera particular, ya que han invertido muchas horas respondiendo preguntas tontas y ayudándonos a pensar mejor acerca de la ciencia de datos:\n\n* Jenny Bryan y Lionel Henry, por las muchas conversaciones útiles en torno\n al trabajo con listas y listas-columnas.\n\n* Los tres capítulos sobre flujo de trabajo fueron adaptados (con permiso)\n de <http://stat545.com/block002_hello-r-workspace-wd-project.html> de\n Jenny Bryan.\n\n* Genevera Allen por las dicusiones acerca de modelos, modelado,\n la perspectiva sobre de aprendizaje estadístico y la diferencia\n entre generación de hipótesis y confirmación de hipótesis.\n\n* Yihui Xie por su trabajo en el paquete [bookdown](https://github.com/rstudio/bookdown),\n y por responder incansablemente las peticiones de nuevas características.\n\n* Bill Behrman por la lectura atenta del libro completo y por probarlo en su\n  curso de Ciencia de Datos en Stanford.\n\n* A la comunidad en Twitter de \\#rstats que revisó todos los borradores de los capítulos\n y ofreció un montón de retroalimentación útil.\n\n* Tal Galili, por aumentar su paquete __dendextend__ para soportar una sección sobre\n _clustering_ que no llegó al borrador final.\n\nEste libro fue escrito de manera abierta y muchas personas contribuyeron con _pull requests_ para resolver problemas pequeños. Especiales agradecimientos para todos quienes contribuyeron via Github:\n\n\nThanks go to all contributers in alphabetical order: Ahmed ElGabbas, Ajay Deonarine, @Alex, Andrew Landgraf, Ben Marwick, Bill Behrman, Brandon Greenwell, Brett Klamer, Christian G. Warden, Christian Mongeau, Colin Gillespie, Cooper Morris, Curtis Alexander, Daniel Gromer, David Clark, Derwin McGeary, Devin Pastoor, Dylan Cashman, Earl Brown, Eric Watt, Etienne B. Racine, Flemming Villalona, Gregory Jefferis, Hengni Cai, Ian Lyttle, Ian Sealy, Jakub Nowosad, Jennifer (Jenny) Bryan, Jeroen Janssens, Jim Hester, Joanne Jang, John Sears, Jon Calder, Jonathan Page, Jose Roberto Ayala Solares, Julia Stewart Lowndes, Julian During, Justinas Petuchovas, Kara Woo, Kenny Darrell, Kirill Sevastyanenko, @KyleHumphrey, Lawrence Wu, @MJMarshall, Matthew Sedaghatfar, Mine Cetinkaya-Rundel, Mustafa Ascha, Nelson Areal, Nick Clark, Nirmal Patel, @OaCantona, Patrick Kennedy, @Paul, Peter Hurford, Rademeyer Vermaak, Radu Grosu, Robert Schuessler, S'busiso Mkhondwane, Shannon Ellis, Steve Mortimer, TJ Mahr, Terence Teo, Thomas Klebel, Tom Prior, Will Beasley, Yihui Xie, adi pradhan, bahadir cankardes, @batpigandme, @behrman, @harrismcgehee, @jennybc, @jjchern, @jonathanflint, @kdpsingh, @koalabearski, @nate-d-olson, @nickelas, @nwaff, @rlzijdeman, @robinlovelace, @robinsones, @seamus-mckinsey, @seanpwilliams, @shoili, @sibusiso16, @spirgel, @svenski, @yahwes, @zeal626.\n\n\n## Colofón\n\nLa versión _online_ de este libro está disponible en <http://es.r4ds.hadley.nz>. El original en inglés puedes encontrarlo en <http://r4ds.had.co.nz>. Las versiones _online_ seguirán evolucionando entre reimpresiones del libro físico.\nLa fuente del libro en español está disponible en <https://github.com/cienciadedatos/r4ds> y la del libro en inglés en <https://github.com/hadley/r4ds>. El libro funciona con <https://bookdown.org>, que hace fácil convertir archivos de R Markdown a HTML, PDF e EPUB.\n\nEste libro fue construido con:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::session_info(c(\"tidyverse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.0 (2023-04-21)\n os       Linux Mint 21.1\n system   x86_64, linux-gnu\n ui       X11\n language en_CA:en\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Toronto\n date     2023-06-06\n pandoc   2.9.2.1 @ /usr/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package       * version    date (UTC) lib source\n askpass         1.1        2019-01-13 [3] CRAN (R 4.0.2)\n backports       1.4.1      2021-12-13 [3] CRAN (R 4.2.0)\n base64enc       0.1-3      2015-07-28 [3] CRAN (R 4.0.2)\n bit             4.0.5      2022-11-15 [1] CRAN (R 4.3.0)\n bit64           4.0.5      2020-08-30 [3] CRAN (R 4.2.0)\n blob            1.2.4      2023-03-17 [3] CRAN (R 4.2.3)\n broom           1.0.4      2023-03-11 [3] CRAN (R 4.2.2)\n bslib           0.4.2      2022-12-16 [1] CRAN (R 4.3.0)\n cachem          1.0.8      2023-05-01 [3] CRAN (R 4.3.0)\n callr           3.7.3      2022-11-02 [3] CRAN (R 4.2.2)\n cellranger      1.1.0      2016-07-27 [3] CRAN (R 4.0.1)\n cli             3.6.1      2023-03-23 [3] CRAN (R 4.2.3)\n clipr           0.8.0      2022-02-22 [3] CRAN (R 4.2.0)\n colorspace      2.1-0      2023-01-23 [3] CRAN (R 4.2.2)\n conflicted      1.2.0      2023-02-01 [3] CRAN (R 4.2.2)\n cpp11         * 0.4.3.9000 2023-05-31 [1] local\n crayon          1.5.2      2022-09-29 [3] CRAN (R 4.2.1)\n curl            5.0.0      2023-01-12 [3] CRAN (R 4.2.2)\n data.table      1.14.8     2023-02-17 [3] CRAN (R 4.2.2)\n DBI             1.1.3      2022-06-18 [3] CRAN (R 4.2.1)\n dbplyr          2.3.2      2023-03-21 [3] CRAN (R 4.2.3)\n digest          0.6.31     2022-12-11 [1] CRAN (R 4.3.0)\n dplyr         * 1.1.2      2023-04-20 [3] CRAN (R 4.3.0)\n dtplyr          1.3.1      2023-03-22 [3] CRAN (R 4.2.3)\n ellipsis        0.3.2      2021-04-29 [3] CRAN (R 4.1.1)\n evaluate        0.21       2023-05-05 [3] CRAN (R 4.3.0)\n fansi           1.0.4      2023-01-22 [3] CRAN (R 4.2.2)\n farver          2.1.1      2022-07-06 [3] CRAN (R 4.2.1)\n fastmap         1.1.1      2023-02-24 [3] CRAN (R 4.2.2)\n fontawesome     0.5.1      2023-04-18 [3] CRAN (R 4.3.0)\n forcats       * 1.0.0      2023-01-29 [3] CRAN (R 4.2.2)\n fs              1.6.2      2023-04-25 [3] CRAN (R 4.3.0)\n gargle          1.4.0      2023-04-15 [3] CRAN (R 4.3.0)\n generics        0.1.3      2022-07-05 [3] CRAN (R 4.2.1)\n ggplot2       * 3.4.2      2023-04-03 [3] CRAN (R 4.3.0)\n glue            1.6.2      2022-02-24 [3] CRAN (R 4.2.0)\n googledrive     2.1.0      2023-03-22 [3] CRAN (R 4.2.3)\n googlesheets4   1.1.0      2023-03-23 [3] CRAN (R 4.2.3)\n gtable          0.3.3      2023-03-21 [3] CRAN (R 4.2.3)\n haven           2.5.2      2023-02-28 [3] CRAN (R 4.2.2)\n highr           0.10       2022-12-22 [1] CRAN (R 4.3.0)\n hms             1.1.3      2023-03-21 [3] CRAN (R 4.2.3)\n htmltools       0.5.5      2023-03-23 [3] CRAN (R 4.2.3)\n httr            1.4.6      2023-05-08 [3] CRAN (R 4.3.0)\n ids             1.0.1      2017-05-31 [3] CRAN (R 4.1.1)\n isoband         0.2.7      2022-12-20 [1] CRAN (R 4.3.0)\n jquerylib       0.1.4      2021-04-26 [3] CRAN (R 4.1.2)\n jsonlite        1.8.5      2023-06-05 [1] CRAN (R 4.3.0)\n knitr           1.42       2023-01-25 [1] CRAN (R 4.3.0)\n labeling        0.4.2      2020-10-20 [3] CRAN (R 4.2.0)\n lattice         0.21-8     2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle       1.0.3      2022-10-07 [3] CRAN (R 4.2.1)\n lubridate     * 1.9.2      2023-02-10 [3] CRAN (R 4.2.2)\n magrittr        2.0.3      2022-03-30 [3] CRAN (R 4.2.0)\n MASS            7.3-60     2023-05-04 [4] CRAN (R 4.3.0)\n Matrix          1.5-4.1    2023-05-18 [1] CRAN (R 4.3.0)\n memoise         2.0.1      2021-11-26 [3] CRAN (R 4.2.0)\n mgcv            1.8-42     2023-03-02 [4] CRAN (R 4.2.3)\n mime            0.12       2021-09-28 [3] CRAN (R 4.2.0)\n modelr          0.1.11     2023-03-22 [3] CRAN (R 4.2.3)\n munsell         0.5.0      2018-06-12 [3] CRAN (R 4.0.1)\n nlme            3.1-162    2023-01-31 [4] CRAN (R 4.2.2)\n openssl         2.0.6      2023-03-09 [3] CRAN (R 4.2.2)\n pillar          1.9.0      2023-03-22 [3] CRAN (R 4.2.3)\n pkgconfig       2.0.3      2019-09-22 [3] CRAN (R 4.0.1)\n prettyunits     1.1.1      2020-01-24 [3] CRAN (R 4.0.1)\n processx        3.8.1      2023-04-18 [3] CRAN (R 4.3.0)\n progress        1.2.2      2019-05-16 [3] CRAN (R 4.0.1)\n ps              1.7.5      2023-04-18 [3] CRAN (R 4.3.0)\n purrr         * 1.0.1      2023-01-10 [3] CRAN (R 4.2.2)\n R6              2.5.1      2021-08-19 [3] CRAN (R 4.2.0)\n ragg            1.2.5      2023-01-12 [1] CRAN (R 4.3.0)\n rappdirs        0.3.3      2021-01-31 [3] CRAN (R 4.2.0)\n RColorBrewer    1.1-3      2022-04-03 [3] CRAN (R 4.2.0)\n readr         * 2.1.4      2023-02-10 [3] CRAN (R 4.2.2)\n readxl          1.4.2      2023-02-09 [3] CRAN (R 4.2.2)\n rematch         1.0.1      2016-04-21 [3] CRAN (R 4.0.1)\n rematch2        2.1.2      2020-05-01 [3] CRAN (R 4.0.1)\n reprex          2.0.2      2022-08-17 [3] CRAN (R 4.2.1)\n rlang           1.1.1      2023-04-28 [3] CRAN (R 4.3.0)\n rmarkdown       2.22       2023-06-01 [3] CRAN (R 4.3.0)\n rstudioapi      0.14       2022-08-22 [3] CRAN (R 4.2.1)\n rvest           1.0.3      2022-08-19 [3] CRAN (R 4.2.1)\n sass            0.4.6      2023-05-03 [3] CRAN (R 4.3.0)\n scales          1.2.1      2022-08-20 [3] CRAN (R 4.2.1)\n selectr         0.4-2      2019-11-20 [3] CRAN (R 4.0.1)\n stringi         1.7.12     2023-01-11 [3] CRAN (R 4.2.2)\n stringr       * 1.5.0      2022-12-02 [3] CRAN (R 4.3.0)\n sys             3.4.2      2023-05-23 [1] CRAN (R 4.3.0)\n systemfonts     1.0.4      2022-02-11 [3] CRAN (R 4.2.0)\n textshaping     0.3.6      2021-10-13 [3] CRAN (R 4.1.1)\n tibble        * 3.2.1      2023-03-20 [3] CRAN (R 4.2.3)\n tidyr         * 1.3.0      2023-01-24 [3] CRAN (R 4.2.2)\n tidyselect      1.2.0      2022-10-10 [3] CRAN (R 4.2.1)\n tidyverse     * 2.0.0      2023-02-22 [3] CRAN (R 4.2.2)\n timechange      0.2.0      2023-01-11 [3] CRAN (R 4.2.2)\n tinytex         0.45       2023-04-18 [3] CRAN (R 4.3.0)\n tzdb            0.4.0      2023-05-12 [3] CRAN (R 4.3.0)\n utf8            1.2.3      2023-01-31 [3] CRAN (R 4.2.2)\n uuid            1.1-0      2022-04-19 [3] CRAN (R 4.2.0)\n vctrs           0.6.2      2023-04-19 [3] CRAN (R 4.3.0)\n viridisLite     0.4.2      2023-05-02 [3] CRAN (R 4.3.0)\n vroom           1.6.3      2023-04-28 [3] CRAN (R 4.3.0)\n withr           2.5.0      2022-03-03 [3] CRAN (R 4.2.0)\n xfun            0.39       2023-04-20 [3] CRAN (R 4.3.0)\n xml2            1.3.4      2023-04-27 [3] CRAN (R 4.3.0)\n yaml            2.3.7      2023-01-23 [3] CRAN (R 4.2.2)\n\n [1] /home/pacha/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}